{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1719740390739,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "lK8-kDv2KPzB"
   },
   "outputs": [],
   "source": [
    "#standard message passing a heterogeneous graph\n",
    "#gcn convultions\n",
    "#gat mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 3292,
     "status": "ok",
     "timestamp": 1719740433435,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "IFadGqY3Kg0D"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import negative_sampling, train_test_split_edges\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, GCNConv, to_hetero, HANConv, Linear\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_name(name):\n",
    "    return re.split(r'[/#]', name)[-1]\n",
    "def read_triples(file_path):\n",
    "    triples = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if len(row) == 3:\n",
    "                triples.append(tuple(preprocess_name(col) for col in row))\n",
    "\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/data_musee/sub_graph.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triples = read_triples(file_path)\n",
    "# entities = set()\n",
    "# relations = set()\n",
    "\n",
    "# for subj, pred, obj in triples:\n",
    "#     entities.add(subj)\n",
    "#     entities.add(obj)\n",
    "#     relations.add(pred)\n",
    "\n",
    "# entity_idx = {name: idx for idx, name in enumerate(entities)}\n",
    "\n",
    "# data['entity'].x = torch.arange(len(entities)).unsqueeze(1).float()\n",
    "\n",
    "# torch.save(data['entity'].x, 'PyG_data/entity_features.pt')\n",
    "\n",
    "# edges = {relation: [] for relation in relations}  # Edge containers for each relation\n",
    "\n",
    "# for subj, pred, obj in triples:\n",
    "#     edges[pred].append((entity_idx[subj], entity_idx[obj]))\n",
    "\n",
    "# # Save edge index tensors using relation indices\n",
    "# relation_mapping = {relation: idx for idx, relation in enumerate(relations)}\n",
    "\n",
    "# for relation, edge_list in edges.items():\n",
    "#     edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "#     relation_index = relation_mapping[relation]\n",
    "#     torch.save(edge_index, f'PyG_data/direct_props/relation_{relation_index}_edge_index.pt')\n",
    "\n",
    "# with open('PyG_data/relation_mapping.csv', 'w') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(['relation', 'index'])\n",
    "#     for relation, index in relation_mapping.items():\n",
    "#         writer.writerow([relation, index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 236409,
     "status": "ok",
     "timestamp": 1719740782160,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "fu6rmTAJKq4z"
   },
   "outputs": [],
   "source": [
    "# loading KG into a Heterogeneous graph\n",
    "\n",
    "data['entity'].x = torch.load('PyG_data/entity_features.pt')\n",
    "\n",
    "\n",
    "relation_mapping = {}\n",
    "with open('PyG_data/relation_mapping.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        relation, index = row\n",
    "        \n",
    "        relation_mapping[relation] = int(index)\n",
    "\n",
    "# Load edge index tensors\n",
    "for relation, index in relation_mapping.items():\n",
    "    edge_index = torch.load(f'PyG_data/direct_props/relation_{index}_edge_index.pt')\n",
    "    data[('entity', relation, 'entity')].edge_index = edge_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  entity={ x=[32153, 1] },\n",
       "  (entity, P1412, entity)={ edge_index=[2, 6] },\n",
       "  (entity, P6379, entity)={ edge_index=[2, 12] },\n",
       "  (entity, P186, entity)={ edge_index=[2, 31457] },\n",
       "  (entity, P119, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P361, entity)={ edge_index=[2, 328] },\n",
       "  (entity, P156, entity)={ edge_index=[2, 17] },\n",
       "  (entity, P195, entity)={ edge_index=[2, 19936] },\n",
       "  (entity, P138, entity)={ edge_index=[2, 5] },\n",
       "  (entity, P69, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P4969, entity)={ edge_index=[2, 106] },\n",
       "  (entity, P840, entity)={ edge_index=[2, 7] },\n",
       "  (entity, P1066, entity)={ edge_index=[2, 8] },\n",
       "  (entity, P410, entity)={ edge_index=[2, 3] },\n",
       "  (entity, P61, entity)={ edge_index=[2, 135] },\n",
       "  (entity, P1196, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P737, entity)={ edge_index=[2, 7] },\n",
       "  (entity, P1433, entity)={ edge_index=[2, 58] },\n",
       "  (entity, P1830, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P562, entity)={ edge_index=[2, 44] },\n",
       "  (entity, P1416, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P1028, entity)={ edge_index=[2, 21] },\n",
       "  (entity, P123, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P1889, entity)={ edge_index=[2, 172] },\n",
       "  (entity, P735, entity)={ edge_index=[2, 8] },\n",
       "  (entity, P1639, entity)={ edge_index=[2, 228] },\n",
       "  (entity, P547, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P825, entity)={ edge_index=[2, 6] },\n",
       "  (entity, P88, entity)={ edge_index=[2, 178] },\n",
       "  (entity, P21, entity)={ edge_index=[2, 7] },\n",
       "  (entity, P166, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P179, entity)={ edge_index=[2, 73] },\n",
       "  (entity, P131, entity)={ edge_index=[2, 195] },\n",
       "  (entity, P412, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P366, entity)={ edge_index=[2, 3] },\n",
       "  (entity, P6275, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P50, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P20, entity)={ edge_index=[2, 7] },\n",
       "  (entity, P6606, entity)={ edge_index=[2, 53] },\n",
       "  (entity, P460, entity)={ edge_index=[2, 4] },\n",
       "  (entity, P1343, entity)={ edge_index=[2, 172] },\n",
       "  (entity, P527, entity)={ edge_index=[2, 218] },\n",
       "  (entity, P25, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P2348, entity)={ edge_index=[2, 286] },\n",
       "  (entity, P941, entity)={ edge_index=[2, 55] },\n",
       "  (entity, P2634, entity)={ edge_index=[2, 12] },\n",
       "  (entity, P607, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P40, entity)={ edge_index=[2, 4] },\n",
       "  (entity, P793, entity)={ edge_index=[2, 200] },\n",
       "  (entity, P407, entity)={ edge_index=[2, 4] },\n",
       "  (entity, P84, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P466, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P7984, entity)={ edge_index=[2, 465] },\n",
       "  (entity, P2682, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P462, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P155, entity)={ edge_index=[2, 3] },\n",
       "  (entity, P805, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P859, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P127, entity)={ edge_index=[2, 3583] },\n",
       "  (entity, P921, entity)={ edge_index=[2, 2887] },\n",
       "  (entity, P5008, entity)={ edge_index=[2, 28] },\n",
       "  (entity, P1071, entity)={ edge_index=[2, 358] },\n",
       "  (entity, P1299, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P189, entity)={ edge_index=[2, 718] },\n",
       "  (entity, P1435, entity)={ edge_index=[2, 31] },\n",
       "  (entity, P149, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P7763, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P1038, entity)={ edge_index=[2, 6] },\n",
       "  (entity, P2936, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P2354, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P276, entity)={ edge_index=[2, 17591] },\n",
       "  (entity, P31, entity)={ edge_index=[2, 19812] },\n",
       "  (entity, P22, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P1950, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P176, entity)={ edge_index=[2, 15] },\n",
       "  (entity, P669, entity)={ edge_index=[2, 7] },\n",
       "  (entity, P9493, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P135, entity)={ edge_index=[2, 1454] },\n",
       "  (entity, P144, entity)={ edge_index=[2, 387] },\n",
       "  (entity, P734, entity)={ edge_index=[2, 6] },\n",
       "  (entity, P770, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P19, entity)={ edge_index=[2, 7] },\n",
       "  (entity, P97, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P106, entity)={ edge_index=[2, 8] },\n",
       "  (entity, P27, entity)={ edge_index=[2, 6] },\n",
       "  (entity, P5816, entity)={ edge_index=[2, 3] },\n",
       "  (entity, P39, entity)={ edge_index=[2, 4] },\n",
       "  (entity, P26, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P170, entity)={ edge_index=[2, 14791] },\n",
       "  (entity, P972, entity)={ edge_index=[2, 134] },\n",
       "  (entity, P1419, entity)={ edge_index=[2, 11] },\n",
       "  (entity, P495, entity)={ edge_index=[2, 265] },\n",
       "  (entity, P2408, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P1303, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P108, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P7108, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P140, entity)={ edge_index=[2, 1521] },\n",
       "  (entity, P509, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P180, entity)={ edge_index=[2, 69953] },\n",
       "  (entity, P2079, entity)={ edge_index=[2, 612] },\n",
       "  (entity, P1877, entity)={ edge_index=[2, 11] },\n",
       "  (entity, P2681, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P2596, entity)={ edge_index=[2, 368] },\n",
       "  (entity, P3373, entity)={ edge_index=[2, 2] },\n",
       "  (entity, P279, entity)={ edge_index=[2, 1] },\n",
       "  (entity, P608, entity)={ edge_index=[2, 2877] },\n",
       "  (entity, P136, entity)={ edge_index=[2, 12323] },\n",
       "  (entity, P17, entity)={ edge_index=[2, 743] },\n",
       "  (entity, P740, entity)={ edge_index=[2, 6] },\n",
       "  (entity, P6216, entity)={ edge_index=[2, 12088] },\n",
       "  (entity, P910, entity)={ edge_index=[2, 7] },\n",
       "  (entity, P1441, entity)={ edge_index=[2, 1] }\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1719740782161,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "zpQZ071Q1yld"
   },
   "outputs": [],
   "source": [
    "def sample_data(data, sample_ratio=0.1):\n",
    "\n",
    "    sampled_data = HeteroData()\n",
    "    sampled_data['entity'].x = data['entity'].x\n",
    "\n",
    "    for edge_type in data.edge_types:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        num_edges = edge_index.size(1)\n",
    "        sample_size = int(num_edges * sample_ratio)\n",
    "        perm = torch.randperm(num_edges)\n",
    "        sampled_indices = perm[:sample_size]\n",
    "        sampled_edge_index = edge_index[:, sampled_indices]\n",
    "        sampled_data[edge_type].edge_index = sampled_edge_index\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1719740782162,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "RCabOrd64BTZ"
   },
   "outputs": [],
   "source": [
    "sample_ratio = 0.1\n",
    "sampled_data = sample_data(data, sample_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1719740782162,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "scCw3sttetsS"
   },
   "outputs": [],
   "source": [
    "# print(\"Edge types before processing:\")\n",
    "# for edge_type in data.edge_types:\n",
    "#     print(edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1719740782162,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "WPngycQYqAu0"
   },
   "outputs": [],
   "source": [
    "def split_edges(edge_index, val_ratio=0.1):\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    perm = torch.randperm(num_edges)\n",
    "    val_size = int(num_edges * val_ratio)\n",
    "    val_indices = perm[:val_size]\n",
    "    train_indices = perm[val_size:]\n",
    "    train_edge_index = edge_index[:, train_indices]\n",
    "    val_edge_index = edge_index[:, val_indices]\n",
    "    return train_edge_index, val_edge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1719740782163,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "Op5-cnXl3_2I"
   },
   "outputs": [],
   "source": [
    "train_data = HeteroData()\n",
    "val_data = HeteroData()\n",
    "\n",
    "train_data['entity'].x = sampled_data['entity'].x\n",
    "val_data['entity'].x = sampled_data['entity'].x\n",
    "\n",
    "for edge_type in sampled_data.edge_types:\n",
    "    edge_index = sampled_data[edge_type].edge_index\n",
    "    train_edge_index, val_edge_index = split_edges(edge_index)\n",
    "    train_data[edge_type].edge_index = train_edge_index\n",
    "    val_data[edge_type].edge_index = val_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(data, num_negatives=1):\n",
    "    negative_edges = {}\n",
    "    for edge_type in data.edge_types:\n",
    "        source_nodes = data[edge_type].edge_index[0]\n",
    "        target_nodes = data[edge_type].edge_index[1]\n",
    "        num_edges = source_nodes.size(0)\n",
    "        \n",
    "        negative_sources = source_nodes.repeat(num_negatives)\n",
    "        negative_targets = torch.randint(0, data['entity'].x.size(0), (num_edges * num_negatives,))\n",
    "        \n",
    "        # Ensure that we are not sampling true edges as negatives\n",
    "        negative_edges[edge_type] = torch.stack([negative_sources, negative_targets], dim=0)\n",
    "        \n",
    "    return negative_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(data, negative_edges):\n",
    "    labels = {}\n",
    "    for edge_type in data.edge_types:\n",
    "        pos_edge_index = data[edge_type].edge_index\n",
    "        neg_edge_index = negative_edges[edge_type]\n",
    "        \n",
    "        # Concatenate positive and negative edges\n",
    "        all_edges = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "        \n",
    "        # Create labels: 1 for positive edges, 0 for negative edges\n",
    "        all_labels = torch.cat([torch.ones(pos_edge_index.size(1)), torch.zeros(neg_edge_index.size(1))])\n",
    "        \n",
    "        labels[edge_type] = all_labels\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                              1.11.0\n",
      "torch-cluster                      1.6.3+pt21cu118\n",
      "torch_geometric                    2.5.3\n",
      "torch-max-mem                      0.1.3\n",
      "torch-ppr                          0.0.8\n",
      "torch-scatter                      2.0.9\n",
      "torchaudio                         2.3.1+cu118\n",
      "torchdata                          0.7.1\n",
      "torchvision                        0.18.1+cu118\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "    def __init__(self, metadata, in_channels, hidden_channels, out_channels, heads=8):\n",
    "        super(HAN, self).__init__()\n",
    "        \n",
    "        # Define the HANConv layers\n",
    "        self.han_conv1 = HANConv(in_channels, hidden_channels, metadata, heads=heads)\n",
    "        self.han_conv2 = HANConv(hidden_channels, out_channels, metadata, heads=heads)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.lin = Linear(out_channels * heads, out_channels)\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Pass through the first HANConv layer\n",
    "        x_dict = self.han_conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.elu(x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Pass through the second HANConv layer\n",
    "        x_dict = self.han_conv2(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.elu(x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Focus on the 'entity' node type for the final layer\n",
    "        x = x_dict['entity']\n",
    "        out = self.lin(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = data.metadata()\n",
    "metadata\n",
    "in_channels = data['entity'].x.size(1)  # Get the number of input features from your entity data\n",
    "model = HAN(metadata, in_channels=in_channels, hidden_channels=64, out_channels=32, heads=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8YXLEhrnmDB"
   },
   "source": [
    "1. built -in conversion from homogeneous GNN model to a heterogeneous GNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1719740782163,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "gaz6Cp2pKjzs"
   },
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "  def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, aggr='mean')\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels, aggr='mean')\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WasfaNr6ouGa"
   },
   "source": [
    "aggregation  : mean (default)\\\n",
    "message : $M_{i,j} = \\frac{x_j \\cdot W}{\\sqrt{D_i \\cdot D_j}}$ \\\n",
    "update : ReLU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1719740782164,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "47jfArG64m-k"
   },
   "outputs": [],
   "source": [
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.linear = torch.nn.Linear(2 * in_channels, 1)\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = torch.cat([x_i, x_j], dim=-1) # Concatenate node embeddings and apply linear layer\n",
    "        return self.linear(x).squeeze(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1719740782164,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "iNQKoKroTwg6"
   },
   "outputs": [],
   "source": [
    "# data.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1719740782164,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "NXHCmBME2_pp"
   },
   "outputs": [],
   "source": [
    "#generate negative samples for each edge type\n",
    "\n",
    "def generate_negatives(data, edge_type, num_neg_samples=None):\n",
    "    # if 'edge_index' not in data[edge_type]:\n",
    "    #     raise AttributeError(f\"Edge type {edge_type} has no edge_index attribute\")\n",
    "\n",
    "    edge_index = data[edge_type].edge_index.cpu()\n",
    "    num_nodes = data['entity'].num_nodes\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=edge_index,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=num_neg_samples,\n",
    "        method='sparse'\n",
    "    )\n",
    "    return neg_edge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1719740782165,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "iZvxxicQPwDb"
   },
   "outputs": [],
   "source": [
    "def get_train_data(data, negative_samples):\n",
    "    pos_edge_index_dict = {}\n",
    "    neg_edge_index_dict = {}\n",
    "\n",
    "    for edge_type in data.edge_types:\n",
    "\n",
    "        pos_edge_index_dict[edge_type] = data[edge_type].edge_index\n",
    "        neg_edge_index_dict[edge_type] = negative_samples[edge_type]\n",
    "\n",
    "    return pos_edge_index_dict, neg_edge_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1719740782165,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "RSQm0rAD1JE5"
   },
   "outputs": [],
   "source": [
    "#training loop\n",
    "\n",
    "def train(model, predictor, data_loader, pos_edge_index_dict, neg_edge_index_dict):\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_dict = model(batch.x_dict, batch.edge_index_dict) #forward pass to update node emmbedding ,processes each edge type separately (conversion)\n",
    "        batch_loss = 0\n",
    "        for edge_type in batch.edge_types:\n",
    "            pos_edge_index = pos_edge_index_dict[edge_type].long()\n",
    "            neg_edge_index = neg_edge_index_dict[edge_type].long()\n",
    "\n",
    "            pos_i = x_dict[edge_type[0]][pos_edge_index[0]] #source nodes\n",
    "            pos_j = x_dict[edge_type[2]][pos_edge_index[1]] #target node\n",
    "\n",
    "            neg_i = x_dict[edge_type[0]][neg_edge_index[0]]\n",
    "            neg_j = x_dict[edge_type[2]][neg_edge_index[1]]\n",
    "\n",
    "            pos_pred = predictor(pos_i, pos_j)\n",
    "            neg_pred = predictor(neg_i, neg_j)\n",
    "\n",
    "            pos_label = torch.ones(pos_pred.size(), device=device)\n",
    "            neg_label = torch.zeros(neg_pred.size(), device=device)\n",
    "\n",
    "            loss = criterion(pos_pred, pos_label) + criterion(neg_pred, neg_label)\n",
    "            batch_loss += loss\n",
    "            \n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1719740783324,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "v9UhU27OqH-0"
   },
   "outputs": [],
   "source": [
    "in_channels = data['entity'].x.size(1)\n",
    "hidden_channels = 64\n",
    "out_channels = 32\n",
    "\n",
    "model = GNN(in_channels, hidden_channels, out_channels)\n",
    "model = to_hetero(model, data.metadata(), aggr='mean')\n",
    "predictor = LinkPredictor(out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1719740783325,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "xs5nbjgcR0XY"
   },
   "outputs": [],
   "source": [
    "negative_samples = {}\n",
    "for edge_type in train_data.edge_types:\n",
    "    neg_edge_index = generate_negatives(train_data, edge_type)\n",
    "    negative_samples[edge_type] = neg_edge_index\n",
    "\n",
    "pos_edge_index_dict, neg_edge_index_dict = get_train_data(train_data, negative_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A40\n",
      "GPU Name: NVIDIA A40\n",
      "GPU Name: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"GPU Name:\", torch.cuda.get_device_name(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1719740783886,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "Lexx6RHYHikY",
    "outputId": "6be6cca1-8023-4268-e30c-fcee906ec900"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "predictor = predictor.to(device)\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "data_loader = DataLoader([train_data], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.0001)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2650,
     "status": "error",
     "timestamp": 1719740786519,
     "user": {
      "displayName": "Hajar Lamtaai",
      "userId": "02388168871005100853"
     },
     "user_tz": -120
    },
    "id": "lmM48O39G_0L",
    "outputId": "20c79b4d-a525-4fe9-bcd6-6050d809a49d"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, predictor, data_loader, pos_edge_index_dict, neg_edge_index_dict)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t53ZXtbFMQp_"
   },
   "source": [
    "### Heterogeneous Convolution Wrapper\n",
    "* define indivudual functions for each type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMBFJUBl6qmCv1UI2GThnlr",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
